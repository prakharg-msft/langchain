{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The request failed with status code: 401\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m\n\u001b[0;32m     31\u001b[0m content_formatter \u001b[39m=\u001b[39m DollyContentFormatter()\n\u001b[0;32m     33\u001b[0m llm \u001b[39m=\u001b[39m AzureMLOnlineEndpoint(\n\u001b[0;32m     34\u001b[0m     endpoint_api_key\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mENDPOINT_API_KEY\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     35\u001b[0m     endpoint_url\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mENDPOINT_URL\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     36\u001b[0m     deployment_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdatabricks-dolly-v2-12b-4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     37\u001b[0m     content_formatter\u001b[39m=\u001b[39mcontent_formatter\n\u001b[0;32m     38\u001b[0m )\n\u001b[1;32m---> 40\u001b[0m resp \u001b[39m=\u001b[39m llm(few_shot_template\u001b[39m.\u001b[39;49mformat(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msoft\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m     41\u001b[0m resp\n",
      "File \u001b[1;32m~\\Developer\\langchain\\langchain\\llms\\base.py:297\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    291\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    294\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m     )\n\u001b[0;32m    296\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 297\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([prompt], stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    298\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    299\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[0;32m    300\u001b[0m )\n",
      "File \u001b[1;32m~\\Developer\\langchain\\langchain\\llms\\base.py:191\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    190\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 191\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    192\u001b[0m run_manager\u001b[39m.\u001b[39mon_llm_end(output)\n\u001b[0;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\Developer\\langchain\\langchain\\llms\\base.py:185\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    180\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    181\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, invocation_params\u001b[39m=\u001b[39mparams\n\u001b[0;32m    182\u001b[0m )\n\u001b[0;32m    183\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m     output \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 185\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    186\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    187\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    190\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32m~\\Developer\\langchain\\langchain\\llms\\base.py:436\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager)\u001b[0m\n\u001b[0;32m    433\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    434\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m    435\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 436\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    438\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m    439\u001b[0m     )\n\u001b[0;32m    440\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[0;32m    441\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\Developer\\langchain\\langchain\\llms\\azureml_endpoint.py:232\u001b[0m, in \u001b[0;36mAzureMLOnlineEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[0;32m    230\u001b[0m body \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent_formatter\u001b[39m.\u001b[39mformat_request_payload(prompt, _model_kwargs)\n\u001b[0;32m    231\u001b[0m endpoint_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhttp_client\u001b[39m.\u001b[39mcall(body)\n\u001b[1;32m--> 232\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontent_formatter\u001b[39m.\u001b[39;49mformat_response_payload(endpoint_response)\n\u001b[0;32m    233\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Developer\\langchain\\langchain\\llms\\azureml_endpoint.py:146\u001b[0m, in \u001b[0;36mDollyContentFormatter.format_response_payload\u001b[1;34m(self, output)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_response_payload\u001b[39m(\u001b[39mself\u001b[39m, output: \u001b[39mbytes\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m--> 146\u001b[0m     response_json \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(output)\n\u001b[0;32m    147\u001b[0m     \u001b[39mreturn\u001b[39;00m response_json[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.llms.azureml_endpoint import AzureMLOnlineEndpoint, DollyContentFormatter\n",
    "import json\n",
    "import os\n",
    "\n",
    "examples = [\n",
    "    {\"word\": \"treasure\", \"antonym\": \"trash\"},\n",
    "    {\"word\": \"loud\", \"antonym\": \"quiet\"},\n",
    "    {\"word\": \"angry\", \"antonym\": \"calm\"},\n",
    "]\n",
    "formatter_template = \"\"\"Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=formatter_template\n",
    ")\n",
    "\n",
    "few_shot_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every word\\n\",\n",
    "    suffix=\"Word: {input}\\nAntonym: \",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "\n",
    "content_formatter = DollyContentFormatter()\n",
    "\n",
    "llm = AzureMLOnlineEndpoint(\n",
    "    endpoint_api_key=os.getenv(\"ENDPOINT_API_KEY\"),\n",
    "    endpoint_url=os.getenv(\"ENDPOINT_URL\"),\n",
    "    deployment_name=\"databricks-dolly-v2-12b-4\",\n",
    "    content_formatter=content_formatter\n",
    ")\n",
    "\n",
    "resp = llm(few_shot_template.format(input=\"soft\"))\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
