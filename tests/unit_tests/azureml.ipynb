{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d639693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.azureml_endpoint import AzureMLModel, LLMBodyHandler\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8890dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BodyHandler(LLMBodyHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    def format_request_payload(self, prompt, model_kwargs) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": {\"input_string\": [prompt]}, \"parameters\": model_kwargs})\n",
    "        return str.encode(input_str)\n",
    "\n",
    "    def format_response_payload(self, output) -> str:\n",
    "        response_json = json.loads(output)\n",
    "        return response_json[0][\"0\"]\n",
    "    \n",
    "body_handler = BodyHandler()\n",
    "\n",
    "azure_llm = AzureMLModel(\n",
    "    endpoint_url=os.getenv(\"ENDPOINT_URL\"),\n",
    "    endpoint_api_key=os.getenv(\"ENDPOINT_API_KEY\"),\n",
    "    deployment_name=\"matthew-gpt-2\",\n",
    "    model_kwargs={\"temperature\": 0.9, \"top_p\": 0.3},\n",
    "    catalog_type=\"open_source\",\n",
    "    body_handler=body_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c788070f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi! How are you doing today?\\n\\n\"Very well, sir! I am doing a lot of reading while I am enjoying your company. Do you mind if I sit here?\\n\\n\"Yes, come here. Sit here, on'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_llm(\"Hi! How are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42f386d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hg_llm = AzureMLModel(\n",
    "    endpoint_url=os.getenv(\"HG_ENDPOINT_URL\"),\n",
    "    endpoint_api_key=os.getenv(\"HG_ENDPOINT_API_KEY\"),\n",
    "    deployment_name=\"eleutherai-pythia-6-9b-5\",\n",
    "    catalog_type=\"hugging_face\",\n",
    "    model_kwargs={\"temperature\": 0.8, \"max_new_tokens\": 50}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2245e74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=\"How are you doing today?\\n\\nI'm doing great. I'm just here to see my friend.\\n\\nOh, okay.\\n\\nI'm just here to see my friend.\\n\\nI'm just here to see my friend.\\n\\nI'm just here\", generation_info=None)], [Generation(text='Why is the sky blue?\\n\\nThe sky is blue because it is made of water.\\n\\nWhy is the sky blue?\\n\\nThe sky is blue because it is made of water.\\n\\nWhy is the sky blue?\\n\\nThe sky is blue because it', generation_info=None)]], llm_output=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg_llm.generate([\"How are you doing today?\", \"Why is the sky blue?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0dbe95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2: \n",
      " Give the antonym of every word\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: light\n",
      "Antonym: dark\n",
      "\n",
      "Word: unique\n",
      "Antonym: similar\n",
      "\n",
      "Word: trash\n",
      "Antonym:  best!\n",
      "------------------------------ \n",
      "Pythia: \n",
      " Give the antonym of every word\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: light\n",
      "Antonym: dark\n",
      "\n",
      "Word: unique\n",
      "Antonym: similar\n",
      "\n",
      "Word: trash\n",
      "Antonym: \n",
      "(I'm not sure if this is the right word, but it's the closest I can think of)\n",
      "\n",
      "A:\n",
      "\n",
      "I think you're looking for the opposite of synonyms.\n",
      "\n",
      "Synonyms are words that\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"light\", \"antonym\": \"dark\"},\n",
    "    {\"word\": \"unique\", \"antonym\": \"similar\"}\n",
    "]\n",
    "formatter_template = \"\"\"Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=formatter_template\n",
    ")\n",
    "\n",
    "few_shot_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every word\\n\",\n",
    "    suffix=\"Word: {input}\\nAntonym: \",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "azure_result = azure_llm(few_shot_template.format(input=\"trash\"))\n",
    "print(\"GPT2: \\n\", azure_result)\n",
    "hg_result = hg_llm(few_shot_template.format(input=\"trash\"))\n",
    "print(\"-\"*30, \"\\nPythia: \\n\", hg_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4abc4782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Come up with a good name for ABC Startup that makes colorful socks. \\xa0The other option is to put the name on the labels and the socks on the socks, but I went the other way for more of a casual feel. \\xa0I'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"company\", \"product\"],\n",
    "    template=\"Come up with a good name for {company} that makes {product}. \",\n",
    ")\n",
    "chain = LLMChain(llm=azure_llm, prompt=prompt)\n",
    "chain.run({\n",
    "    'company': \"ABC Startup\",\n",
    "    'product': \"colorful socks\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79999df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(hg_llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4b4c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is thirteen squared?\u001b[32;1m\u001b[1;3mTranslate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${Question with math problem.}\n",
      "```text\n",
      "${single line mathematical expression that solves the problem}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${Output of running the code}\n",
      "```\n",
      "Answer: ${Answer}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: What is thirteen squared?\n",
      "```text\n",
      "13^2\n",
      "```\n",
      "...numexpr.evaluate(\"13^2\")...\n",
      "```output\n",
      "49\n",
      "```\n",
      "Answer: 49\n",
      "\n",
      "Question: What is the square root of $2^{10}$?\n",
      "```text\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer:  49\\n\\nQuestion: What is the square root of $2^{10}$?\\n```text'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math.run(\"What is thirteen squared?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3620d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
