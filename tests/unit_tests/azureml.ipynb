{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d639693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.azureml_endpoint import AzureMLModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8890dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azure_llm = AzureMLModel(\n",
    "    endpoint_url=os.getenv(\"ENDPOINT_URL\"),\n",
    "    endpoint_api_key=os.getenv(\"ENDPOINT_API_KEY\"),\n",
    "    deployment_name=\"matthew-gpt-2\",\n",
    "    model_kwargs={\"temperature\": 0.9, \"top_p\": 0.3, \"max_tokens\": 500},\n",
    "    catalog_type=\"open_source\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c788070f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi! How are you doing today?\\n\\nP: Sorry, are you sure? (laugh)\\n\\nH: Sorry, I've only just landed here. My flight was delayed for 10 minutes.\\n\\nP: I can understand that\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_llm(\"Hi! How are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f386d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hg_llm = AzureMLModel(\n",
    "    endpoint_url=os.getenv(\"HG_ENDPOINT_URL\"),\n",
    "    endpoint_api_key=os.getenv(\"HG_ENDPOINT_API_KEY\"),\n",
    "    deployment_name=\"eleutherai-pythia-6-9b-5\",\n",
    "    catalog_type=\"hugging_face\",\n",
    "    model_kwargs={\"temperature\": 0.8, \"max_new_tokens\": 50}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2245e74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=\"How are you doing today?\\n\\nI'm doing great. I'm just here to see my friend.\\n\\nOh, okay.\\n\\nI'm just here to see my friend.\\n\\nI'm just here to see my friend.\\n\\nI'm just here\", generation_info=None)], [Generation(text='Why is the sky blue?\\n\\nThe sky is blue because it is made of water.\\n\\nWhy is the sky blue?\\n\\nThe sky is blue because it is made of water.\\n\\nWhy is the sky blue?\\n\\nThe sky is blue because it', generation_info=None)]], llm_output=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg_llm.generate([\"How are you doing today?\", \"Why is the sky blue?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0dbe95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2: \n",
      " Give the antonym of every word\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: light\n",
      "Antonym: dark\n",
      "\n",
      "Word: unique\n",
      "Antonym: similar\n",
      "\n",
      "Word: trash\n",
      "Antonym: urn\n",
      "\n",
      "------------------------------ \n",
      "Pythia: \n",
      " Give the antonym of every word\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: light\n",
      "Antonym: dark\n",
      "\n",
      "Word: unique\n",
      "Antonym: similar\n",
      "\n",
      "Word: trash\n",
      "Antonym: \n",
      "(I'm not sure if this is the right word, but it's the closest I can think of)\n",
      "\n",
      "A:\n",
      "\n",
      "I think you're looking for the opposite of synonyms.\n",
      "\n",
      "Synonyms are words that\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"light\", \"antonym\": \"dark\"},\n",
    "    {\"word\": \"unique\", \"antonym\": \"similar\"}\n",
    "]\n",
    "formatter_template = \"\"\"Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=formatter_template\n",
    ")\n",
    "\n",
    "few_shot_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every word\\n\",\n",
    "    suffix=\"Word: {input}\\nAntonym: \",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "azure_result = azure_llm(few_shot_template.format(input=\"trash\"))\n",
    "print(\"GPT2: \\n\", azure_result)\n",
    "hg_result = hg_llm(few_shot_template.format(input=\"trash\"))\n",
    "print(\"-\"*30, \"\\nPythia: \\n\", hg_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4abc4782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Come up with a good name for ABC Startup that makes colorful socks. \\xa0A lot of things will happen. \\xa0I mean, if we have an \"A\" for Startup, we would probably just call it \"Founded by the Alphabet'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"company\", \"product\"],\n",
    "    template=\"Come up with a good name for {company} that makes {product}. \",\n",
    ")\n",
    "chain = LLMChain(llm=azure_llm, prompt=prompt)\n",
    "chain.run({\n",
    "    'company': \"ABC Startup\",\n",
    "    'product': \"colorful socks\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79999df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(azure_llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4b4c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is thirteen squared?\u001b[32;1m\u001b[1;3mTranslate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${Question with math problem.}\n",
      "```text\n",
      "${single line mathematical expression that solves the problem}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${Output of running the code}\n",
      "```\n",
      "Answer: ${Answer}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: What is thirteen squared?\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer:  8.222831614237718\\n\\nQuestion: What is thirteen squared?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math.run(\"What is thirteen squared?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bff60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
