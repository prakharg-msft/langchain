{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d639693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.azureml_endpoint import AzureMLModel, LLMBodyHandler\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8890dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class OSSBodyHandler(LLMBodyHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    def format_request_payload(self, prompt, model_kwargs) -> bytes:\n",
    "        input_str = json.dumps({\"inpus\": {\"input_string\": [prompt]}, \"parameters\": model_kwargs})\n",
    "        return str.encode(input_str)\n",
    "\n",
    "    def format_response_payload(self, output) -> str:\n",
    "        response_json = json.loads(output)\n",
    "        return response_json[0][\"0\"]\n",
    "\n",
    "azure_llm = AzureMLModel(\n",
    "#     endpoint_url=os.getenv(\"ENDPOINT_URL\"),\n",
    "    endpoint_url=\"https://bing.com\",\n",
    "    endpoint_api_key=os.getenv(\"ENDPOINT_API_KEY\"),\n",
    "    deployment_name=\"matthew-gpt-2\",\n",
    "    model_kwargs={\"temperature\": 0.9, \"top_p\": 0.3},\n",
    "    body_handler=OSSBodyHandler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c788070f",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mazure_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi! How are you doing today?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Developer\\langchain\\langchain\\llms\\base.py:297\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m     )\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    300\u001b[0m )\n",
      "File \u001b[1;32m~\\Developer\\langchain\\langchain\\llms\\base.py:191\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    192\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\Developer\\langchain\\langchain\\llms\\base.py:185\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[0;32m    180\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    181\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}, prompts, invocation_params\u001b[38;5;241m=\u001b[39mparams\n\u001b[0;32m    182\u001b[0m )\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 185\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32m~\\Developer\\langchain\\langchain\\llms\\base.py:436\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager)\u001b[0m\n\u001b[0;32m    433\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m    435\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 436\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    439\u001b[0m     )\n\u001b[0;32m    440\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\Developer\\langchain\\langchain\\llms\\azureml_endpoint.py:196\u001b[0m, in \u001b[0;36mAzureMLModel._call\u001b[1;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[0;32m    194\u001b[0m body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody_handler\u001b[38;5;241m.\u001b[39mformat_request_payload(prompt, _model_kwargs)\n\u001b[0;32m    195\u001b[0m endpoint_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client\u001b[38;5;241m.\u001b[39mcall(body)\n\u001b[1;32m--> 196\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_response_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# TODO: Add error handling\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m, in \u001b[0;36mOSSBodyHandler.format_response_payload\u001b[1;34m(self, output)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_response_payload\u001b[39m(\u001b[38;5;28mself\u001b[39m, output) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     response_json \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response_json[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "azure_llm(\"Hi! How are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f386d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class HGBodyHandler(LLMBodyHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    def format_request_payload(self, prompt, model_kwargs) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": [prompt], \"parameters\": model_kwargs})\n",
    "        return str.encode(input_str)\n",
    "\n",
    "    def format_response_payload(self, output) -> str:\n",
    "        response_json = json.loads(output)\n",
    "        return response_json[0][0][\"generated_text\"]\n",
    "\n",
    "hg_llm = AzureMLModel(\n",
    "    endpoint_url=os.getenv(\"HG_ENDPOINT_URL\"),\n",
    "    endpoint_api_key=os.getenv(\"HG_ENDPOINT_API_KEY\"),\n",
    "    deployment_name=\"eleutherai-pythia-6-9b-5\",\n",
    "    catalog_type=\"hugging_face\",\n",
    "    model_kwargs={\"temperature\": 0.8, \"max_new_tokens\": 50},\n",
    "    body_handler=HGBodyHandler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2245e74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=\"How are you doing today?\\n\\nI'm doing great. I'm just here to see my friend.\\n\\nOh, okay.\\n\\nI'm just here to see my friend.\\n\\nI'm just here to see my friend.\\n\\nI'm just here\", generation_info=None)], [Generation(text='Why is the sky blue?\\n\\nThe sky is blue because it is made of water.\\n\\nWhy is the sky blue?\\n\\nThe sky is blue because it is made of water.\\n\\nWhy is the sky blue?\\n\\nThe sky is blue because it', generation_info=None)]], llm_output=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg_llm.generate([\"How are you doing today?\", \"Why is the sky blue?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0dbe95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2: \n",
      " Give the antonym of every word\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: light\n",
      "Antonym: dark\n",
      "\n",
      "Word: unique\n",
      "Antonym: similar\n",
      "\n",
      "Word: trash\n",
      "Antonym: icky\n",
      "\n",
      "------------------------------ \n",
      "Pythia: \n",
      " Give the antonym of every word\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: light\n",
      "Antonym: dark\n",
      "\n",
      "Word: unique\n",
      "Antonym: similar\n",
      "\n",
      "Word: trash\n",
      "Antonym: \n",
      "(I'm not sure if this is the right word, but it's the closest I can think of)\n",
      "\n",
      "A:\n",
      "\n",
      "I think you're looking for the opposite of synonyms.\n",
      "\n",
      "Synonyms are words that\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"light\", \"antonym\": \"dark\"},\n",
    "    {\"word\": \"unique\", \"antonym\": \"similar\"}\n",
    "]\n",
    "formatter_template = \"\"\"Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=formatter_template\n",
    ")\n",
    "\n",
    "few_shot_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every word\\n\",\n",
    "    suffix=\"Word: {input}\\nAntonym: \",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "azure_result = azure_llm(few_shot_template.format(input=\"trash\"))\n",
    "print(\"GPT2: \\n\", azure_result)\n",
    "hg_result = hg_llm(few_shot_template.format(input=\"trash\"))\n",
    "print(\"-\"*30, \"\\nPythia: \\n\", hg_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4abc4782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Come up with a good name for ABC Startup that makes colorful socks. \\xa0The other option is to put the name on the labels and the socks on the socks, but I went the other way for more of a casual feel. \\xa0I'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"company\", \"product\"],\n",
    "    template=\"Come up with a good name for {company} that makes {product}. \",\n",
    ")\n",
    "chain = LLMChain(llm=azure_llm, prompt=prompt)\n",
    "chain.run({\n",
    "    'company': \"ABC Startup\",\n",
    "    'product': \"colorful socks\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79999df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(hg_llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4b4c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is thirteen squared?\u001b[32;1m\u001b[1;3mTranslate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${Question with math problem.}\n",
      "```text\n",
      "${single line mathematical expression that solves the problem}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${Output of running the code}\n",
      "```\n",
      "Answer: ${Answer}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: What is thirteen squared?\n",
      "```text\n",
      "13^2\n",
      "```\n",
      "...numexpr.evaluate(\"13^2\")...\n",
      "```output\n",
      "49\n",
      "```\n",
      "Answer: 49\n",
      "\n",
      "Question: What is the square root of $2^{10}$?\n",
      "```text\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer:  49\\n\\nQuestion: What is the square root of $2^{10}$?\\n```text'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math.run(\"What is thirteen squared?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3620d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
